{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91631e56",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Text as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31314614",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb4222",
   "metadata": {},
   "source": [
    "### Text is High Dimensional\n",
    "- Suppose that there is a sample of documents, each document is $n_L$ words long, drawn from vocabulary of $n_V$ words\n",
    "- Then an unique representation of each document has dimensionality (**in information theory, the number of possible states or the size of the data sapce**) ${n_V}^{n_L}$\n",
    "- For example, a representation of a 30-word Twitter message using only the one thousand most common words will have dimensionality $={1000}^{30}$\n",
    "- Therefore, due to the curse of dimensionality (dimensionality grows expotentially as feature number, e.g. the word count of a document, increases), raw text is hard to be used as a input variable for prediction or causal inference.\n",
    "- Instead, some procedures are needed to retain useful information from the text and compress its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad15ee",
   "metadata": {},
   "source": [
    "### Common Procedure of Text Analysis\n",
    "1. Convert raw text $D$ to a numerical array $C$ (e.g. the elements of $C$ can be counts over tokens or binary indicators of the positions of tokens)\n",
    "2. Map C to predicted values $\\hat{V}$ of unknown outcomes $V$ (the information that is assumed to be \"useful\")\n",
    "3. The $\\hat{V}$ is learned through supervised ML for labeled $C_i$ and $V_i$ or unsupervised ML for unlabeled $C_i$ (e.g. learn topics or principal dimensions)\n",
    "4. Finally use $\\hat{V}$ for subsequent descriptive or causal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98584cd7",
   "metadata": {},
   "source": [
    "### Basic Concepts of Text Analysis\n",
    "- **Document:** a sequence of characters, the fundamental unit of analysis; \n",
    "    - the determination of document, however, will vary depending on our question; \n",
    "    - for example, we may count a pharagrpah as a document or a whole article as a document\n",
    "- **Corpus:** the set of documents\n",
    "- **Tokens:** words or phrases\n",
    "- **Unstructured data:** data such that useful information is mixed with lots of useless information\n",
    "    - text data is unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e4728",
   "metadata": {},
   "source": [
    "### Relating Text to Metadata\n",
    "- Text data (documents) are not that meaningful by themselves; they are useful only if related to metadata.\n",
    "- For example, measuring positive-negative sentiment $Y_i$ in each political speeches is not that meaningful by itself.\n",
    "- But we can relate sentiment $Y_{ijkt}$ to the information of politician $j$, topic $k$, and time $t$; these are metadata (data that is not from the text).\n",
    "- Then we can look at interesting question such as: does sentiment vary over time $t$ or does politician $j$ express more negative sentiment toward topic $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301c147",
   "metadata": {},
   "source": [
    "### Text Analysis ML Methods\n",
    "There are many methods that can be used to map raw text (documents) to some more compressed objects and retain \"useful\" information from them:\n",
    "- Dictionaries, Tokenization, and Document Distance\n",
    "- Topic Models and ML with text, Word Embeddings and Linguistic Parsing\n",
    "- Transformers and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d22e5",
   "metadata": {},
   "source": [
    "## Quantity (Text Length) as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa2710",
   "metadata": {},
   "source": [
    "### Judge Age and Writing Style\n",
    "The paper is written by Ash, Goessmann, and MacLeod (2022):\n",
    "- They count average word length and sentence length from documents written by different judges\n",
    "- They regress average word length and sentence length on age of judges.\n",
    "- There is a negative correlation between average word length and judge age (elders tend to use simplier words)\n",
    "- There is a positive correlation between average sentence length and judge age (elders tend to use more complex sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e6e20",
   "metadata": {},
   "source": [
    "### Optimal Legal Complexity\n",
    "The paper is written by Katz and Bommarito (2014):\n",
    "- **Motivation:**\n",
    "    - More legal detail added to laws, more properly they specified rules and target incentives to activities and groups\n",
    "    - But there are costs to understanding, following, and maintaining complex laws\n",
    "    - Given the trade-off between complexity and readability, there should exist an optimal legal complexity\n",
    "- **Methodology:**\n",
    "    - Katz and Bommarito measure legal complexity using number of words and also word entropy (diversity of the vocabulary in the law, or predictivity of the writting)\n",
    "    - Word entropy: \n",
    "        - for a corpus treated as a set of sequences of words, let $V$ denote the vocabulary\n",
    "        - let $p(w)$ be the empirical probability of observing word $w$ in each document: $$ p(w)= \\frac{count(w)}{\\sum_{v \\in V}{count(v)}} $$\n",
    "        - $count(v)$ is the count of occurence of word $v$ in the document\n",
    "        - word entropy (for each document) is defined as: $$ H=-\\sum_{w \\in V}{p(w){log}_{2}(p(w))} $$\n",
    "        - this measure gets larger if there are many words appear with small probability (higher diversity)\n",
    "- **Results:**\n",
    "    - They rank law titles by word number and word entropy\n",
    "    - They found that Public Health and Welfare, Conservation, and Commerce and Trade have both high word number and entropy (most complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cd81e",
   "metadata": {},
   "source": [
    "## Dictionary-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962316c",
   "metadata": {},
   "source": [
    "### Overview of Dictionary-Based Methods\n",
    "- Dictionary-based methods use dictionary (a pre-selected list of words or phrases) to analyze a corpus (map document to more compressed objects and retain useful information)\n",
    "- These methods identify patterns or counts defined by the dictionary using regular expressions\n",
    "- These methods are corpus-specific, that is, they identify sets of words or phrases across documents of a corpus in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3413974",
   "metadata": {},
   "source": [
    "### Example 1: Measuring Uncertainty in Macroeconomy\n",
    "The paper is written by Baker, Bloom, and Davis (2016)\n",
    "- Methodology:\n",
    "    - Filter each newspaper on each day since 1985 by:\n",
    "        - Article contains “uncertain” or “uncertainty”\n",
    "        - Article contains “economic” or “economy”\n",
    "        - Article contains “congress” or “deficit” or “federal reserve” or “legislation” or “regulation” or “white house”\n",
    "    - Normalize resulting article counts by total newspaper articles that month to create a news-based economic policy uncertainty index\n",
    "- Result:\n",
    "    - They found that the uncertainty index peaked during economic crisis in history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3fe96",
   "metadata": {},
   "source": [
    "### Example 2: Identifying Race-Related Research in Economics\n",
    "- Motivation:\n",
    "    - How does economics compare to other social sciences in study of race-related issues?\n",
    "- Methodology:\n",
    "    - Corpus:\n",
    "        - Considered all journals that JSTOR characterizes as comprising the disciplines of economics, sociology, and political science\n",
    "        - Created a corpus of publications from 1960 to 2020: 224,855 publications from 231 economics journals, 138,188 publications from 185 sociology journals, and 110,835 publications from 213 political science journals\n",
    "    - Dictionary:\n",
    "        - The list of keywords are created along two dimensions: (i) the racial or ethnic group being studied; and (ii) the issue being studied\n",
    "        - Examples of keywords along the group dimension are race, african-american, person of color, and ethnicity\n",
    "        - Examples of issue keywords include discrimination, prejudice, and stereotype\n",
    "    - Identifying method:\n",
    "        - A publication is identified as race-related if: \n",
    "            - (i) at least one group keyword is in the title\n",
    "            - (ii) at least one group keyword and at least one issue keyword are mentioned in the title or abstract\n",
    "        - For rule (ii) they drop the last sentence of the abstract to avoid false positives from research that only mentions race parenthetically (robustness check rather than the primary focus of study)\n",
    "    - Further categorization:\n",
    "        - Band 0 consists of generic keywords denoting racial and ethnic groups (e.g. race, ethnic, under represented minority)\n",
    "        - Band 1 adds group keywords relating to the main minority groups in the U.S. (e.g. African American, Latinos and Native Americans)\n",
    "        - Band 2 adds less salient group keywords (e.g. White, South Asian, Indian American, Japanese American) and other minorities based on religious beliefs (e.g. Muslim, Jewish).\n",
    "        - Words and phrases are also broadly split across five broader topics: discrimination, inequality, diversity, identity, and historical issues\n",
    "- Results:\n",
    "    - The share of race-issue related publications from economics are far behind political science and sociology for decades\n",
    "    - The weighted number of race-related publications by journal quality is also dominated by sociology, political science, and finally economics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cc463",
   "metadata": {},
   "source": [
    "### General Dictionaries\n",
    "Researchers can either use self-defined dictioary or dictionary defined by others; below are some common pre-defined dictionaries:\n",
    "- **WordNet:**\n",
    "    - It is an English word database with 118K nouns, 12K verbs, 22K adjectives, 5K adverbs\n",
    "    - It contains:\n",
    "        - Snonym sets (synsets): a group of near-synonyms, plus a gloss (definition), e.g. good - great\n",
    "        - Antonyms (opposites): e.g. good - bad\n",
    "        - Holonyms/meronyms (part-whole): e.g. leaf - tree\n",
    "        - And different meanings (senses) of the word\n",
    "    - Nouns are organized in categorical hierarchy (that is why it is called WordNet):\n",
    "        - “hypernym”: the higher category that a word is a member of, e.g. motion\n",
    "        - \"hyponyms”: members of the category identified by a word, e.g. walking, flying, swimming\n",
    "- **Stopwords:**\n",
    "    - Also called function words, these are words that does not contain much information\n",
    "    - Include words such as for, rather and than\n",
    "    - Removing them can help us to get at non-topical dimensions (include more information)\n",
    "- **Linguistic Inquiry and Word Counts (LIWC):**\n",
    "    - $>$ 10000 words from $>$ 100 lists of category-relevant words\n",
    "    - Categories include: “emotion”, “cognition”, “work”, “family”, “positive”, “negative” , etc.\n",
    "- **Emotional words:**\n",
    "    - Mohammad and Turney (2011) coded 10,000 words along four emotional dimensions\n",
    "        - These are joy–sadness, anger-fear, trust-disgust, anticipation-surprise\n",
    "    - Warriner et al (2013) coded 14,000 words along three emotional dimensions\n",
    "        - These are valence, arousal, dominance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d87ef",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7d195",
   "metadata": {},
   "source": [
    "### Overview of Sentiment Analysis\n",
    "- The aim is to extracting a “tone” dimension from the document: such as positive, negative, and neutral\n",
    "- The standard approach is lexicon (dictionary) based:\n",
    "    - e.g. if \"good\" appears, it is positive\n",
    "    - but they fail easily: e.g., “good” versus “not good” versus \"not very good\"\n",
    "- Alternative approaches are transformer-based sentiment models (HuggingFace)\n",
    "    - However, these methods can still introduce bias if the model is trained on biased corpora\n",
    "        - For example, if the transformer model is trained on online writing (informal)\n",
    "        - It may not work for legal text (formal)\n",
    "    - Besides, there are ethical concern over supervised NLP models:\n",
    "        - Some times specific names or countries can induce change in sentiment score (e.g. typically white name increases sentiment, typically black name decreases it)\n",
    "        - This is because sentiment models that are trained on annotated datasets also learn from the correlated non-sentiment information\n",
    "        - For example, in the training set maybe people complain about Mexican food more often than Italian food because Italian restaurants tend to be more upscale\n",
    "        - This will make the model confounded by the correlation between \"Mexican\" and negative sentiment, though \"Mexican\" has no real implication on sentiment score\n",
    "        - (supervised models learn features that are correlated with the label being annotated)\n",
    "    - Dictionary methods, while having other limitations, mitigate this problem\n",
    "        - The researcher intentionally “regularizes” out spurious confounders with the targeted language dimension\n",
    "        - Helps explain why economists often still use dictionary methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
